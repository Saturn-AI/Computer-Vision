{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0c4fa",
   "metadata": {},
   "source": [
    "Executing_Example_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4923fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=2,\n",
    "    min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_3d = np.array([\n",
    "    [0.0, 0.0, 0.0],            # Nose tip\n",
    "    [0.0, -330.0, -65.0],       # Chin\n",
    "    [-225.0, 170.0, -135.0],    # Left eye left corner\n",
    "    [225.0, 170.0, -135.0],     # Right eye right corner\n",
    "    [-150.0, -150.0, -125.0],   # Left Mouth corner\n",
    "    [150.0, -150.0, -125.0]     # Right mouth corner\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# Reposition left eye corner to be the origin\n",
    "leye_3d = np.array(face_3d)\n",
    "leye_3d[:,0] += 225\n",
    "leye_3d[:,1] -= 175\n",
    "leye_3d[:,2] += 135\n",
    "\n",
    "# Reposition right eye corner to be the origin\n",
    "reye_3d = np.array(face_3d)\n",
    "reye_3d[:,0] -= 225\n",
    "reye_3d[:,1] -= 175\n",
    "reye_3d[:,2] += 135\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue \n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "        \n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate left x gaze score\n",
    "        if (face_2d[243,0] - face_2d[130,0]) != 0:\n",
    "            lx_score = (face_2d[468,0] - face_2d[130,0]) / (face_2d[243,0] - face_2d[130,0])\n",
    "            if abs(lx_score - last_lx) < threshold:\n",
    "                lx_score = (lx_score + last_lx) / 2\n",
    "            last_lx = lx_score\n",
    "\n",
    "        # Calculate left y gaze score\n",
    "        if (face_2d[23,1] - face_2d[27,1]) != 0:\n",
    "            ly_score = (face_2d[468,1] - face_2d[27,1]) / (face_2d[23,1] - face_2d[27,1])\n",
    "            if abs(ly_score - last_ly) < threshold:\n",
    "                ly_score = (ly_score + last_ly) / 2\n",
    "            last_ly = ly_score\n",
    "\n",
    "        # Calculate right x gaze score\n",
    "        if (face_2d[359,0] - face_2d[463,0]) != 0:\n",
    "            rx_score = (face_2d[473,0] - face_2d[463,0]) / (face_2d[359,0] - face_2d[463,0])\n",
    "            if abs(rx_score - last_rx) < threshold:\n",
    "                rx_score = (rx_score + last_rx) / 2\n",
    "            last_rx = rx_score\n",
    "\n",
    "        # Calculate right y gaze score\n",
    "        if (face_2d[253,1] - face_2d[257,1]) != 0:\n",
    "            ry_score = (face_2d[473,1] - face_2d[257,1]) / (face_2d[253,1] - face_2d[257,1])\n",
    "            if abs(ry_score - last_ry) < threshold:\n",
    "                ry_score = (ry_score + last_ry) / 2\n",
    "            last_ry = ry_score\n",
    "\n",
    "        # The camera matrix\n",
    "        focal_length = 1 * img_w\n",
    "        cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                [0, focal_length, img_w / 2],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "        # Distortion coefficients \n",
    "        dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "        # Solve PnP\n",
    "        _, l_rvec, l_tvec = cv2.solvePnP(leye_3d, face_2d_head, cam_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "        _, r_rvec, r_tvec = cv2.solvePnP(reye_3d, face_2d_head, cam_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "\n",
    "        # Get rotational matrix from rotational vector\n",
    "        l_rmat, _ = cv2.Rodrigues(l_rvec)\n",
    "        r_rmat, _ = cv2.Rodrigues(r_rvec)\n",
    "\n",
    "\n",
    "        # [0] changes pitch\n",
    "        # [1] changes roll\n",
    "        # [2] changes yaw\n",
    "        # +1 changes ~45 degrees (pitch down, roll tilts left (counterclockwise), yaw spins left (counterclockwise))\n",
    "\n",
    "        # Adjust headpose vector with gaze score\n",
    "        l_gaze_rvec = np.array(l_rvec)\n",
    "        l_gaze_rvec[2][0] -= (lx_score-.5) * x_score_multiplier\n",
    "        l_gaze_rvec[0][0] += (ly_score-.5) * y_score_multiplier\n",
    "\n",
    "        r_gaze_rvec = np.array(r_rvec)\n",
    "        r_gaze_rvec[2][0] -= (rx_score-.5) * x_score_multiplier\n",
    "        r_gaze_rvec[0][0] += (ry_score-.5) * y_score_multiplier\n",
    "\n",
    "        # --- Projection ---\n",
    "\n",
    "        # Get left eye corner as integer\n",
    "        l_corner = face_2d_head[2].astype(np.int32)\n",
    "\n",
    "        # Project axis of rotation for left eye\n",
    "        axis = np.float32([[-100, 0, 0], [0, 100, 0], [0, 0, 300]]).reshape(-1, 3)\n",
    "        l_axis, _ = cv2.projectPoints(axis, l_rvec, l_tvec, cam_matrix, dist_coeffs)\n",
    "        l_gaze_axis, _ = cv2.projectPoints(axis, l_gaze_rvec, l_tvec, cam_matrix, dist_coeffs)\n",
    "\n",
    "        # Draw axis of rotation for left eye\n",
    "        if draw_headpose:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_axis[0]).astype(np.int32)), (200,200,0), 3)\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_axis[1]).astype(np.int32)), (0,200,0), 3)\n",
    "            cv2.line(img, l_corner, tuple(np.ravel(l_axis[2]).astype(np.int32)), (0,200,200), 3)\n",
    "\n",
    "        if draw_gaze:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[0]).astype(np.int32)), (255,0,0), 3)\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[1]).astype(np.int32)), (0,255,0), 3)\n",
    "            cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[2]).astype(np.int32)), (0,0,255), 3)\n",
    "\n",
    "        \n",
    "    \n",
    "        # Get left eye corner as integer\n",
    "        r_corner = face_2d_head[3].astype(np.int32)\n",
    "\n",
    "        # Get left eye corner as integer\n",
    "        r_axis, _ = cv2.projectPoints(axis, r_rvec, r_tvec, cam_matrix, dist_coeffs)\n",
    "        r_gaze_axis, _ = cv2.projectPoints(axis, r_gaze_rvec, r_tvec, cam_matrix, dist_coeffs)\n",
    "\n",
    "        # Draw axis of rotation for left eye\n",
    "        if draw_headpose:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_axis[0]).astype(np.int32)), (200,200,0), 3)\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_axis[1]).astype(np.int32)), (0,200,0), 3)\n",
    "            cv2.line(img, r_corner, tuple(np.ravel(r_axis[2]).astype(np.int32)), (0,200,200), 3)\n",
    "\n",
    "        if draw_gaze:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[0]).astype(np.int32)), (255,0,0), 3)\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[1]).astype(np.int32)), (0,255,0), 3)\n",
    "            cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[2]).astype(np.int32)), (0,0,255), 3)\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow('Head Pose Estimation', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15d937",
   "metadata": {},
   "source": [
    "# Build zone Based Gaze Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f44d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Mesh for landmark detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "\n",
    "# Open the video capture (camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the initial width and height of the captured video frame\n",
    "img_w, img_h = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Define parameters for visualization and analysis\n",
    "draw_gaze = True  # Set to True to draw gaze vectors\n",
    "draw_full_axis = False  # Set to True to draw full headpose axis\n",
    "draw_headpose = True  # Set to True to draw headpose angles\n",
    "\n",
    "# Multiplier for Gaze Score (affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold for how close scores should be to the average between frames\n",
    "threshold = .3\n",
    "\n",
    "# Initialize the MediaPipe Face Mesh module\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize image width and height\n",
    "img_w, img_h = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Define coordinates for different zones on the screen\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "}\n",
    "\n",
    "# Initialize colors for different zones\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "# Initialize variables to store the last gaze coordinates\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Main loop for capturing and processing video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video capture\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip and convert the image from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Process the face mesh to get facial landmarks\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Extract 2D face landmarks if available\n",
    "    face_2d = []\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Determine the current zone based on the location of a specific landmark\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[159][0] <= x2 and y1 <= face_2d[159][1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Reset zone colors and highlight the current zone\n",
    "        for zone in zone_coordinates.keys():\n",
    "            zones_color[zone] = (255, 255, 255)\n",
    "\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)\n",
    "\n",
    "        # Draw rectangles around different zones\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)\n",
    "\n",
    "    # Display the processed frame with zone information\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
