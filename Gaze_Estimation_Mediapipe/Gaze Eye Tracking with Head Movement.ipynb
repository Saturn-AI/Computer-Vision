{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d3e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\AlphaN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4923fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=2,\n",
    "    min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_3d = np.array([\n",
    "    [0.0, 0.0, 0.0],            # Nose tip\n",
    "    [0.0, -330.0, -65.0],       # Chin\n",
    "    [-225.0, 170.0, -135.0],    # Left eye left corner\n",
    "    [225.0, 170.0, -135.0],     # Right eye right corner\n",
    "    [-150.0, -150.0, -125.0],   # Left Mouth corner\n",
    "    [150.0, -150.0, -125.0]     # Right mouth corner\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "# Reposition left eye corner to be the origin\n",
    "leye_3d = np.array(face_3d)\n",
    "leye_3d[:,0] += 225\n",
    "leye_3d[:,1] -= 175\n",
    "leye_3d[:,2] += 135\n",
    "\n",
    "# Reposition right eye corner to be the origin\n",
    "reye_3d = np.array(face_3d)\n",
    "reye_3d[:,0] -= 225\n",
    "reye_3d[:,1] -= 175\n",
    "reye_3d[:,2] += 135\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue \n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "        \n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate left x gaze score\n",
    "        if (face_2d[243,0] - face_2d[130,0]) != 0:\n",
    "            lx_score = (face_2d[468,0] - face_2d[130,0]) / (face_2d[243,0] - face_2d[130,0])\n",
    "            if abs(lx_score - last_lx) < threshold:\n",
    "                lx_score = (lx_score + last_lx) / 2\n",
    "            last_lx = lx_score\n",
    "\n",
    "        # Calculate left y gaze score\n",
    "        if (face_2d[23,1] - face_2d[27,1]) != 0:\n",
    "            ly_score = (face_2d[468,1] - face_2d[27,1]) / (face_2d[23,1] - face_2d[27,1])\n",
    "            if abs(ly_score - last_ly) < threshold:\n",
    "                ly_score = (ly_score + last_ly) / 2\n",
    "            last_ly = ly_score\n",
    "\n",
    "        # Calculate right x gaze score\n",
    "        if (face_2d[359,0] - face_2d[463,0]) != 0:\n",
    "            rx_score = (face_2d[473,0] - face_2d[463,0]) / (face_2d[359,0] - face_2d[463,0])\n",
    "            if abs(rx_score - last_rx) < threshold:\n",
    "                rx_score = (rx_score + last_rx) / 2\n",
    "            last_rx = rx_score\n",
    "\n",
    "        # Calculate right y gaze score\n",
    "        if (face_2d[253,1] - face_2d[257,1]) != 0:\n",
    "            ry_score = (face_2d[473,1] - face_2d[257,1]) / (face_2d[253,1] - face_2d[257,1])\n",
    "            if abs(ry_score - last_ry) < threshold:\n",
    "                ry_score = (ry_score + last_ry) / 2\n",
    "            last_ry = ry_score\n",
    "\n",
    "        # The camera matrix\n",
    "        focal_length = 1 * img_w\n",
    "        cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                [0, focal_length, img_w / 2],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "        # Distortion coefficients \n",
    "        dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "        # Solve PnP\n",
    "        _, l_rvec, l_tvec = cv2.solvePnP(leye_3d, face_2d_head, cam_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "        _, r_rvec, r_tvec = cv2.solvePnP(reye_3d, face_2d_head, cam_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "\n",
    "        # Get rotational matrix from rotational vector\n",
    "        l_rmat, _ = cv2.Rodrigues(l_rvec)\n",
    "        r_rmat, _ = cv2.Rodrigues(r_rvec)\n",
    "\n",
    "\n",
    "        # [0] changes pitch\n",
    "        # [1] changes roll\n",
    "        # [2] changes yaw\n",
    "        # +1 changes ~45 degrees (pitch down, roll tilts left (counterclockwise), yaw spins left (counterclockwise))\n",
    "\n",
    "        # Adjust headpose vector with gaze score\n",
    "        l_gaze_rvec = np.array(l_rvec)\n",
    "        l_gaze_rvec[2][0] -= (lx_score-.5) * x_score_multiplier\n",
    "        l_gaze_rvec[0][0] += (ly_score-.5) * y_score_multiplier\n",
    "\n",
    "        r_gaze_rvec = np.array(r_rvec)\n",
    "        r_gaze_rvec[2][0] -= (rx_score-.5) * x_score_multiplier\n",
    "        r_gaze_rvec[0][0] += (ry_score-.5) * y_score_multiplier\n",
    "\n",
    "        # --- Projection ---\n",
    "\n",
    "        # Get left eye corner as integer\n",
    "        l_corner = face_2d_head[2].astype(np.int32)\n",
    "\n",
    "        # Project axis of rotation for left eye\n",
    "        axis = np.float32([[-100, 0, 0], [0, 100, 0], [0, 0, 300]]).reshape(-1, 3)\n",
    "        l_axis, _ = cv2.projectPoints(axis, l_rvec, l_tvec, cam_matrix, dist_coeffs)\n",
    "        l_gaze_axis, _ = cv2.projectPoints(axis, l_gaze_rvec, l_tvec, cam_matrix, dist_coeffs)\n",
    "\n",
    "        # Draw axis of rotation for left eye\n",
    "        if draw_headpose:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_axis[0]).astype(np.int32)), (200,200,0), 3)\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_axis[1]).astype(np.int32)), (0,200,0), 3)\n",
    "            cv2.line(img, l_corner, tuple(np.ravel(l_axis[2]).astype(np.int32)), (0,200,200), 3)\n",
    "\n",
    "        if draw_gaze:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[0]).astype(np.int32)), (255,0,0), 3)\n",
    "                cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[1]).astype(np.int32)), (0,255,0), 3)\n",
    "            cv2.line(img, l_corner, tuple(np.ravel(l_gaze_axis[2]).astype(np.int32)), (0,0,255), 3)\n",
    "\n",
    "        \n",
    "    \n",
    "        # Get left eye corner as integer\n",
    "        r_corner = face_2d_head[3].astype(np.int32)\n",
    "\n",
    "        # Get left eye corner as integer\n",
    "        r_axis, _ = cv2.projectPoints(axis, r_rvec, r_tvec, cam_matrix, dist_coeffs)\n",
    "        r_gaze_axis, _ = cv2.projectPoints(axis, r_gaze_rvec, r_tvec, cam_matrix, dist_coeffs)\n",
    "\n",
    "        # Draw axis of rotation for left eye\n",
    "        if draw_headpose:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_axis[0]).astype(np.int32)), (200,200,0), 3)\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_axis[1]).astype(np.int32)), (0,200,0), 3)\n",
    "            cv2.line(img, r_corner, tuple(np.ravel(r_axis[2]).astype(np.int32)), (0,200,200), 3)\n",
    "\n",
    "        if draw_gaze:\n",
    "            if draw_full_axis:\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[0]).astype(np.int32)), (255,0,0), 3)\n",
    "                cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[1]).astype(np.int32)), (0,255,0), 3)\n",
    "            cv2.line(img, r_corner, tuple(np.ravel(r_gaze_axis[2]).astype(np.int32)), (0,0,255), 3)\n",
    "                \n",
    "\n",
    "\n",
    "    cv2.imshow('Head Pose Estimation', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_right': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'top_left': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_left': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (0, 0, 0) for zone in zone_coordinates}\n",
    "\n",
    "# ...\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "    # Determine the current zone based on gaze scores\n",
    "    current_zone = None\n",
    "    for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "        if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "            current_zone = zone\n",
    "            break\n",
    "\n",
    "    # Update the color of the current zone to red\n",
    "    if current_zone is not None:\n",
    "        zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "    # Draw rectangles for each zone\n",
    "    for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], -1)  # Fill rectangle\n",
    "\n",
    "    # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    # Handle key press to exit the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bab8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaze and Zone Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, img)\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;66;03m# Handle key press to exit the loop\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Release the video capture and close all windows\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_3d = np.array([\n",
    "    [0.0, 0.0, 0.0],            # Nose tip\n",
    "    [0.0, -330.0, -65.0],       # Chin\n",
    "    [-225.0, 170.0, -135.0],    # Left eye left corner\n",
    "    [225.0, 170.0, -135.0],     # Right eye right corner\n",
    "    [-150.0, -150.0, -125.0],   # Left Mouth corner\n",
    "    [150.0, -150.0, -125.0]     # Right mouth corner\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Reposition left eye corner to be the origin\n",
    "leye_3d = np.array(face_3d)\n",
    "leye_3d[:, 0] += 225\n",
    "leye_3d[:, 1] -= 175\n",
    "leye_3d[:, 2] += 135\n",
    "\n",
    "# Reposition right eye corner to be the origin\n",
    "reye_3d = np.array(face_3d)\n",
    "reye_3d[:, 0] -= 225\n",
    "reye_3d[:, 1] -= 175\n",
    "reye_3d[:, 2] += 135\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_right': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'top_left': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_left': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (0, 0, 0) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], -1)  # Fill rectangle\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32a5e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m last_ly, last_ry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Define the coordinates for the zones\u001b[39;00m\n\u001b[0;32m     54\u001b[0m zone_coordinates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[43mimg_w\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w, img_h),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h),\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     60\u001b[0m }\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Initialize the zones color dictionary\u001b[39;00m\n\u001b[0;32m     63\u001b[0m zones_color \u001b[38;5;241m=\u001b[39m {zone: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m zone \u001b[38;5;129;01min\u001b[39;00m zone_coordinates}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_w' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_3d = np.array([\n",
    "    [0.0, 0.0, 0.0],            # Nose tip\n",
    "    [0.0, -330.0, -65.0],       # Chin\n",
    "    [-225.0, 170.0, -135.0],    # Left eye left corner\n",
    "    [225.0, 170.0, -135.0],     # Right eye right corner\n",
    "    [-150.0, -150.0, -125.0],   # Left Mouth corner\n",
    "    [150.0, -150.0, -125.0]     # Right mouth corner\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Reposition left eye corner to be the origin\n",
    "leye_3d = np.array(face_3d)\n",
    "leye_3d[:, 0] += 225\n",
    "leye_3d[:, 1] -= 175\n",
    "leye_3d[:, 2] += 135\n",
    "\n",
    "# Reposition right eye corner to be the origin\n",
    "reye_3d = np.array(face_3d)\n",
    "reye_3d[:, 0] -= 225\n",
    "reye_3d[:, 1] -= 175\n",
    "reye_3d[:, 2] += 135\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_right': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'top_left': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_left': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (0, 0, 0) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], -1)  # Fill rectangle\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], -1)  # Fill rectangle\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cefab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m last_ly, last_ry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Define the coordinates for the zones\u001b[39;00m\n\u001b[0;32m     33\u001b[0m zone_coordinates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[43mimg_w\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w, img_h),\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h),\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Initialize the zones color dictionary\u001b[39;00m\n\u001b[0;32m     42\u001b[0m zones_color \u001b[38;5;241m=\u001b[39m {zone: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m zone \u001b[38;5;129;01min\u001b[39;00m zone_coordinates}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_w' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_right': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'top_left': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_left': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (0, 0, 0) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], -1)  # Fill rectangle\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Reset the colors for each zone to the original state (white)\n",
    "    zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)  # Hollow rectangle, 2-pixel thickness\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a2c5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m last_ly, last_ry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Define the coordinates for the zones\u001b[39;00m\n\u001b[0;32m     33\u001b[0m zone_coordinates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[43mimg_w\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h),\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w, img_h),\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Initialize the zones color dictionary\u001b[39;00m\n\u001b[0;32m     42\u001b[0m zones_color \u001b[38;5;241m=\u001b[39m {zone: (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m zone \u001b[38;5;129;01min\u001b[39;00m zone_coordinates}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_w' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gaze scores from the previous frame\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    (img_h, img_w, img_c) = img.shape  # Initialize img_w and img_h here\n",
    "\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            # Convert landmark x and y to pixel coordinates\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "            # Add the 2D coordinates to an array\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        # Get relevant landmarks for headpose estimation\n",
    "        face_2d_head = np.array([\n",
    "            face_2d[1],      # Nose\n",
    "            face_2d[199],    # Chin\n",
    "            face_2d[33],     # Left eye left corner\n",
    "            face_2d[263],    # Right eye right corner\n",
    "            face_2d[61],     # Left mouth corner\n",
    "            face_2d[291]     # Right mouth corner\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        face_2d = np.asarray(face_2d)\n",
    "\n",
    "        # Calculate gaze scores (as in your existing code)\n",
    "\n",
    "        # Determine the current zone based on gaze scores\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[468, 0] <= x2 and y1 <= face_2d[468, 1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        # Update the color of the current zone to red\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)  # Red\n",
    "\n",
    "        # Draw rectangles for each zone\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)  # Hollow rectangle, 2-pixel thickness\n",
    "\n",
    "        # Draw the existing head pose and gaze lines (as in your existing code)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "        # Handle key press to exit the loop\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15d937",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a4047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\AlphaN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f44d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = True\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize img_w and img_h\n",
    "img_w, img_h = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    #'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    face_2d = []\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        face_2d = []\n",
    "        for idx, lm in enumerate(face_landmarks.landmark):\n",
    "            x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "            face_2d.append((x, y))\n",
    "\n",
    "        current_zone = None\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            if x1 <= face_2d[159][0] <= x2 and y1 <= face_2d[159][1] <= y2:\n",
    "                current_zone = zone\n",
    "                break\n",
    "\n",
    "        for zone in zone_coordinates.keys():\n",
    "            zones_color[zone] = (255, 255, 255)\n",
    "\n",
    "        if current_zone is not None:\n",
    "            zones_color[current_zone] = (0, 0, 255)\n",
    "\n",
    "        for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)\n",
    "\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d938a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\AlphaN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = .3\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                                  refine_landmarks=True,\n",
    "                                  max_num_faces=2,\n",
    "                                  min_detection_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "img_w, img_h = int(cap.get(3)), int(cap.get(4))\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "last_lx, last_rx = 0, 0\n",
    "last_ly, last_ry = 0, 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            face_2d = []\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "                face_2d.append((x, y))\n",
    "\n",
    "            # Check if relevant landmarks are present\n",
    "            if len(face_2d) >= 468:\n",
    "                # Extract eye landmark positions\n",
    "                left_eye = (face_2d[159][0], face_2d[159][1])\n",
    "                right_eye = (face_2d[386][0], face_2d[386][1])\n",
    "\n",
    "                # Calculate gaze vector (from left eye to right eye)\n",
    "                gaze_vector = (right_eye[0] - left_eye[0], right_eye[1] - left_eye[1])\n",
    "\n",
    "                # Calculate the angle of the gaze vector\n",
    "                gaze_angle = np.degrees(np.arctan2(gaze_vector[1], gaze_vector[0]))\n",
    "\n",
    "                # Visualize the gaze direction as a line\n",
    "                line_length = 50\n",
    "                line_end = (\n",
    "                    int(left_eye[0] + line_length * np.cos(np.radians(gaze_angle))),\n",
    "                    int(left_eye[1] + line_length * np.sin(np.radians(gaze_angle)))\n",
    "                )\n",
    "                cv2.line(img, left_eye, line_end, (0, 255, 0), 2)\n",
    "\n",
    "                # Determine the current zone based on the gaze angle\n",
    "                current_zone = None\n",
    "                if -45 < gaze_angle <= 45:\n",
    "                    current_zone = 'right'\n",
    "                elif 45 < gaze_angle <= 135:\n",
    "                    current_zone = 'top'\n",
    "                elif -135 <= gaze_angle <= -45:\n",
    "                    current_zone = 'bottom'\n",
    "                else:\n",
    "                    current_zone = 'left'\n",
    "\n",
    "                # Update zone colors based on gaze\n",
    "                for zone in zone_coordinates.keys():\n",
    "                    zones_color[zone] = (255, 255, 255)\n",
    "\n",
    "                if current_zone is not None:\n",
    "                    zones_color[current_zone] = (0, 0, 255)\n",
    "\n",
    "                # Draw rectangles on zones\n",
    "                for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)\n",
    "\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ... (rest of the code)\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e033541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\AlphaN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Define the coordinates for the zones\u001b[39;00m\n\u001b[0;32m     20\u001b[0m zone_coordinates \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[43mimg_w\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_right\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, img_w, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_left\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, img_w, img_h),\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m: (img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m img_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     26\u001b[0m }\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Initialize the zones color dictionary\u001b[39;00m\n\u001b[0;32m     29\u001b[0m zones_color \u001b[38;5;241m=\u001b[39m {zone: (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m zone \u001b[38;5;129;01min\u001b[39;00m zone_coordinates}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_w' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "############## PARAMETERS #######################################################\n",
    "\n",
    "# Set these values to show/hide certain vectors of the estimation\n",
    "draw_gaze = True\n",
    "draw_full_axis = False\n",
    "draw_headpose = False\n",
    "\n",
    "# Gaze Score multiplier (Higher multiplier = Gaze affects headpose estimation more)\n",
    "x_score_multiplier = 10\n",
    "y_score_multiplier = 10\n",
    "\n",
    "# Threshold of how close scores should be to average between frames\n",
    "threshold = 0.3\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=2,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            face_2d = []\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "                face_2d.append((x, y))\n",
    "\n",
    "            if len(face_2d) >= 468:\n",
    "                left_eye = (face_2d[159][0], face_2d[159][1])\n",
    "                right_eye = (face_2d[386][0], face_2d[386][1])\n",
    "\n",
    "                gaze_vector = (right_eye[0] - left_eye[0], right_eye[1] - left_eye[1])\n",
    "                gaze_angle = np.degrees(np.arctan2(gaze_vector[1], gaze_vector[0]))\n",
    "\n",
    "                # Visualize the gaze direction as a line\n",
    "                line_length = 50\n",
    "                line_end = (\n",
    "                    int(left_eye[0] + line_length * np.cos(np.radians(gaze_angle))),\n",
    "                    int(left_eye[1] + line_length * np.sin(np.radians(gaze_angle)))\n",
    "                )\n",
    "                cv2.line(img, left_eye, line_end, (0, 255, 0), 2)\n",
    "\n",
    "                # Determine the current zone based on the gaze angle\n",
    "                current_zone = None\n",
    "                if -45 < gaze_angle <= 45:\n",
    "                    current_zone = 'right'\n",
    "                elif 45 < gaze_angle <= 135:\n",
    "                    current_zone = 'top'\n",
    "                elif -135 <= gaze_angle <= -45:\n",
    "                    current_zone = 'bottom'\n",
    "                else:\n",
    "                    current_zone = 'left'\n",
    "\n",
    "                # Update zone colors based on gaze\n",
    "                for zone in zone_coordinates.keys():\n",
    "                    zones_color[zone] = (255, 255, 255)\n",
    "\n",
    "                if current_zone is not None:\n",
    "                    zones_color[current_zone] = (0, 0, 255)\n",
    "\n",
    "                # Draw rectangles on zones\n",
    "                for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)\n",
    "\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7478a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=2,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the coordinates for the zones\n",
    "zone_coordinates = {\n",
    "    'top_right': (0, 0, img_w // 2, img_h // 2),\n",
    "    'bottom_right': (0, img_h // 2, img_w // 2, img_h),\n",
    "    'top_left': (img_w // 2, 0, img_w, img_h // 2),\n",
    "    'bottom_left': (img_w // 2, img_h // 2, img_w, img_h),\n",
    "    'center': (img_w // 4, img_h // 4, 3 * img_w // 4, 3 * img_h // 4)\n",
    "}\n",
    "\n",
    "# Initialize the zones color dictionary\n",
    "zones_color = {zone: (255, 255, 255) for zone in zone_coordinates}\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip + convert img from BGR to RGB\n",
    "    img = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get the result\n",
    "    results = face_mesh.process(img)\n",
    "    img.flags.writeable = True\n",
    "\n",
    "    # Convert the color space from RGB to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            face_2d = []\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "                face_2d.append((x, y))\n",
    "\n",
    "            if len(face_2d) >= 468:\n",
    "                left_eye = (face_2d[159][0], face_2d[159][1])\n",
    "                right_eye = (face_2d[386][0], face_2d[386][1])\n",
    "\n",
    "                gaze_vector = (right_eye[0] - left_eye[0], right_eye[1] - left_eye[1])\n",
    "                gaze_angle = np.degrees(np.arctan2(gaze_vector[1], gaze_vector[0]))\n",
    "\n",
    "                # Determine the current zone based on the gaze angle\n",
    "                current_zone = None\n",
    "                if -45 < gaze_angle <= 45:\n",
    "                    current_zone = 'right'\n",
    "                elif 45 < gaze_angle <= 135:\n",
    "                    current_zone = 'top'\n",
    "                elif -135 <= gaze_angle <= -45:\n",
    "                    current_zone = 'bottom'\n",
    "                else:\n",
    "                    current_zone = 'left'\n",
    "\n",
    "                # Update zone colors based on gaze\n",
    "                for zone in zone_coordinates.keys():\n",
    "                    zones_color[zone] = (255, 255, 255)\n",
    "\n",
    "                if current_zone is not None:\n",
    "                    zones_color[current_zone] = (0, 0, 255)\n",
    "\n",
    "                # Draw rectangles on zones\n",
    "                for zone, (x1, y1, x2, y2) in zone_coordinates.items():\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), zones_color[zone], 2)\n",
    "\n",
    "    cv2.imshow('Gaze and Zone Detection', img)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3309e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
